
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: loki
  name: loki
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: loki
  name: loki
rules:
- apiGroups:
  - extensions
  resourceNames:
  - loki
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: loki
  name: loki
  namespace: logging
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: loki
subjects:
- kind: ServiceAccount
  name: loki
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-cm
data:
  loki.yaml: |-
    auth_enabled: false
    
    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
    
    ingester:
      lifecycler:
        address: 0.0.0.0
        ring:
          replication_factor: 1
          kvstore:
            store: inmemory
        final_sleep: "0s"
      wal:
        enabled: false    
      chunk_idle_period: 1h       
      max_chunk_age: 1h           
      chunk_target_size: 1048576  
      chunk_retain_period: 30s    
      max_transfer_retries: 0    
    
    schema_config:
      configs:
        - from: "2020-05-15"
          store: boltdb-shipper
          object_store: s3
          schema: v11
          index:
            period: 24h
            prefix: index_
    
    storage_config:
      aws:
        bucketnames: lokik8sminio
        endpoint: http://minio-svc.minio-store.svc.cluster.local:9000
        insecure: true
        s3: null
        region: null
        s3forcepathstyle: true
        access_key_id: admin
        secret_access_key: admin2675
      boltdb_shipper:
        active_index_directory: /data/loki/index
        cache_location: /data/loki/cache
        cache_ttl: 24h
        shared_store: s3
    
    compactor:
      compaction_interval: 5m
      shared_store: s3
      working_directory: /data/loki/boltdb-shipper-compactor
    
    limits_config:
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_global_streams_per_user: 10000       # for error sending batch, Too Many Requests, Ingestion rate limit exceeded  
    
    chunk_store_config:
      max_look_back_period: 0s
    
    table_manager:
      retention_deletes_enabled: false
      retention_period: 0s
    
    ruler:
      storage:
        type: local
        local:
          directory: /etc/loki/rules
      rule_path: /temp/loki/rules
      ring:
        kvstore:
          store: inmemory
      alertmanager_url: http://kubemon-alertmanager.monitoring.svc.cluster.local:9093
      enable_api: true
      enable_alertmanager_v2: true
    
    # If you would like to disable reporting, uncomment the following lines:
    #analytics:
    #  reporting_enabled: false
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubemon-loki-alert-config
  labels:
    app.kubernetes.io/name: kubemon
    app.kubernetes.io/component: prometheus
data:
  rules.yml: |-
    "groups":
    - name: apps-rules
      rules:
      - alert: AppsErrorlog
        expr: sum by (pod,namespace)(count_over_time({pod!=""}|~ "[Ee]rror"[1m])) >= 1
        for: 1m
        labels:
          severity: warning
          category: logs
        annotations:
          title: "Application error in logs"
          messages: "Found error in POD ({{ $labels.pod }}) in namespace ({{ $labels.namespace }}) is above 1 (current value: {{ $value }})"
      - alert: NoSuchHostError
        expr: sum by (pod,namespace)(count_over_time({pod !~ "(kubemon-alertmanager-0|kubemon-alertmanager-1|loki-0|loki-1)",namespace !~ "(logging|monitoring)"}|~ "no such host"[1m])) >= 1
        for: 1m
        labels:
          severity: warning
          category: logs
        annotations:
          title: "No Such Host Error"
          messages: "No Such Host error found in POD ({{ $labels.pod }}) in namespace ({{ $labels.namespace }}) is above 1 (current value: {{ $value }})"
      - alert: OfflinPODError
        expr: sum by (pod,namespace)(count_over_time({pod !~ "(kubemon-alertmanager-0|kubemon-alertmanager-1|loki-0|loki-1)",namespace !~ "(logging|monitoring)"}|~ "1 Offline"[1m])) >= 1
        for: 1m
        labels:
          severity: warning
          category: logs
        annotations:
          title: "Offline POD Error"
          messages: "Offline POD error found in POD ({{ $labels.pod }}) in namespace ({{ $labels.namespace }}) is above 1 (current value: {{ $value }})"          
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: loki
  name: loki
spec:
  ports:
  - name: http-metrics
    port: 3100
    protocol: TCP
    targetPort: http-metrics
  selector:
    app: loki
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: loki
  name: loki-headless
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 3100
    protocol: TCP
    targetPort: http-metrics
  selector:
    app: loki
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: loki
  name: loki
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: loki
  serviceName: loki-headless
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - args:
        - -config.file=/etc/loki/loki.yaml
        #image: docker.io/grafana/loki:2.0.0-amd64
        image: docker.io/grafana/loki:2.8.0       
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /ready
            port: http-metrics
            scheme: HTTP
          initialDelaySeconds: 45
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: loki
        ports:
        - containerPort: 3100
          name: http-metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /ready
            port: http-metrics
            scheme: HTTP
          initialDelaySeconds: 45
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        securityContext:
          procMount: Default
          readOnlyRootFilesystem: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/loki
          name: config
        - mountPath: /etc/loki/rules
          name: alert
        - mountPath: /data
          name: storage
        - mountPath: /temp
          name: temp
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      serviceAccount: loki
      serviceAccountName: loki
      terminationGracePeriodSeconds: 30
      volumes:
      - name: config
        configMap:
          defaultMode: 420
          name: loki-cm
      - name: alert
        configMap:
          name: kubemon-loki-alert-config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: temp
  updateStrategy:
    type: RollingUpdate
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: minio-provisioner-lokik8sminio
  namespace: logging
  annotations:
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded  
    argocd.argoproj.io/sync-wave: "-2"  
  labels:
    app.kubernetes.io/instance: minio-configurator
    app.kubernetes.io/name: minio-configurator
---
apiVersion: v1
kind: Secret
metadata:
  name: minio-provisioner-lokik8sminio
  namespace: logging
  annotations:
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded  
    argocd.argoproj.io/sync-wave: "-2"
type: Opaque
data:
  # By default user admin and password admin2675
  access_key: YWRtaW4=
  secret_key: "YWRtaW4yNjc1"
  # By default user global and password global2675
  common_user: Z2xvYmFs
  common_pass: "Z2xvYmFsMjY3NQ=="
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: minio-provisioner-lokik8sminio
  namespace: logging
  annotations:
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded  
    argocd.argoproj.io/sync-wave: "-2"
  labels:
    app.kubernetes.io/instance: minio-configurator
    app.kubernetes.io/name: minio-configurator
data:
  bucket-rules.json: |
    {
      "Rules": [
        {
          "ID": "bucket-retention",
          "Status": "Enabled",
          "Expiration": {
            "Days": 7
          }
          ,
          "NoncurrentVersionExpiration": {
            "NoncurrentDays": 3
          }
        }
      ]
    }
  bucket-policy.json: |
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Deny",
          "Action": ["s3:GetBucketLocation","s3:ListBucket","s3:ListBucketMultipartUploads"],
          "Resource": ["arn:aws:s3:::lokik8sminio"]
        },
        {
          "Effect": "Allow",
          "Action": ["s3:AbortMultipartUpload","s3:DeleteObject","s3:GetObject","s3:ListMultipartUploadParts","s3:PutObject"],
          "Resource": ["arn:aws:s3:::lokik8sminio/*"]
        }
      ]
    }
---
apiVersion: batch/v1
kind: Job
metadata:
  name: minio-provisioner
  namespace: logging
  annotations:
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
    argocd.argoproj.io/sync-wave: "1"
  labels:
    app.kubernetes.io/instance: minio-configurator
    app.kubernetes.io/name: minio-configurator
spec:
  # Automatic Cleanup for Finished Jobs
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
    spec:
      containers:
      - command:
        - /bin/bash
        - -c
        - |-
          set -e;
          echo "Start Minio provisioning";
          function attachPolicy() {
            local tmp=$(mc admin $1 info provisioning $2 | sed -n -e 's/^Policy.*: \(.*\)$/\1/p');
            IFS=',' read -r -a CURRENT_POLICIES <<< "$tmp";
            if [[ ! "${CURRENT_POLICIES[*]}" =~ "$3" ]]; then
              mc admin policy attach provisioning $3 --$1=$2;
            fi;
          };
          function detachDanglingPolicies() {
            local tmp=$(mc admin $1 info provisioning $2 | sed -n -e 's/^Policy.*: \(.*\)$/\1/p');
            IFS=',' read -r -a CURRENT_POLICIES <<< "$tmp";
            IFS=',' read -r -a DESIRED_POLICIES <<< "$3";
            for current in "${CURRENT_POLICIES[@]}"; do
              if [[ ! "${DESIRED_POLICIES[*]}" =~ "${current}" ]]; then
                mc admin policy detach provisioning $current --$1=$2;
              fi;
            done;
          }
          function addUsersFromFile() {
            local username=$(grep -oP '^username=\K.+' $1);
            local password=$(grep -oP '^password=\K.+' $1);
            local disabled=$(grep -oP '^disabled=\K.+' $1);
            local policies_list=$(grep -oP '^policies=\K.+' $1);
            local set_policies=$(grep -oP '^setPolicies=\K.+' $1);

            mc admin user add provisioning "${username}" "${password}";

            IFS=',' read -r -a POLICIES <<< "${policies_list}";
            for policy in "${POLICIES[@]}"; do
              attachPolicy user "${username}" "${policy}";
            done;
            if [ "${set_policies}" == "true" ]; then
              detachDanglingPolicies user "${username}" "${policies_list}";
            fi;

            local user_status="enable";
            if [[ "${disabled}" != "" && "${disabled,,}" == "true" ]]; then
              user_status="disable";
            fi;

            mc admin user "${user_status}" provisioning "${username}";
          }; mc alias set provisioning $MINIO_SCHEME://$MINIO_HOST:$MINIO_PORT $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD;
          mc admin config set provisioning region name=minio; mc admin policy create provisioning lokik8sminio-bucket-policy /etc/ilm/bucket-policy.json;
          mc admin user add provisioning $MINIO_GLOBAL_USER $MINIO_GLOBAL_PASSWORD;
          attachPolicy user $MINIO_GLOBAL_USER readwrite;
          attachPolicy user $MINIO_GLOBAL_USER consoleAdmin;
          attachPolicy user $MINIO_GLOBAL_USER diagnostics;
          mc admin user enable provisioning $MINIO_GLOBAL_USER;
          #mc mb provisioning/lokik8sminio --ignore-existing --region=minio --with-lock;
          mc mb provisioning/lokik8sminio --ignore-existing --region=minio;          
          mc ilm import provisioning/lokik8sminio < /etc/ilm/bucket-rules.json;
          mc version suspend provisioning/lokik8sminio;          
          mc quota set provisioning/lokik8sminio --size $MINIO_QUOTA;
          mc tag set provisioning/lokik8sminio "name=logging";
          #mc retention set --default GOVERNANCE $MINIO_RETENTION provisioning/lokik8sminio;
          echo "End Minio provisioning";
        env:
        - name: MINIO_QUOTA
          value: "10GiB"
        - name: MINIO_RETENTION
          value: "5d"
        - name: MC_INSECURE
          value: "true"
        - name: MINIO_SCHEME
          value: http
        - name: MINIO_HOST
          value: "minio-svc.minio-store.svc.cluster.local"
        - name: MINIO_PORT
          value: "9000"
        - name: MINIO_ROOT_USER
          valueFrom:
            secretKeyRef:
              name: minio-provisioner-lokik8sminio
              key: access_key
        - name: MINIO_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: minio-provisioner-lokik8sminio
              key: secret_key
        - name: MINIO_GLOBAL_USER
          valueFrom:
            secretKeyRef:
              name: minio-provisioner-lokik8sminio
              key: common_user
        - name: MINIO_GLOBAL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: minio-provisioner-lokik8sminio
              key: common_pass
        image: docker.io/bitnami/minio:2024.5.1-debian-12-r0
        imagePullPolicy: IfNotPresent
        name: minio
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /.mc
          name: empty-dir
          subPath: app-mc-dir
        - mountPath: /opt/bitnami/minio/tmp
          name: empty-dir
          subPath: app-tmp-dir
        - mountPath: /tmp
          name: empty-dir
          subPath: tmp-dir
        - mountPath: /etc/ilm
          name: minio-provisioning
        - mountPath: /opt/bitnami/minio/secrets/
          name: minio-credentials
      dnsPolicy: ClusterFirst
      initContainers:
      - command:
        - /bin/bash
        - -c
        - |-
          set -e;
          echo "Waiting for Minio";
          wait-for-port \
            --host=$MINIO_HOST \
            --state=inuse \
            --timeout=120 \
            $MINIO_PORT;
          echo "Minio is available";
        image: docker.io/bitnami/minio:2024.5.1-debian-12-r0
        imagePullPolicy: IfNotPresent
        name: wait-for-available-minio
        env:
        - name: MINIO_HOST
          value: "minio-svc.minio-store.svc.cluster.local"
        - name: MINIO_PORT
          value: "9000"
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      restartPolicy: OnFailure
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: minio-provisioner-lokik8sminio
      serviceAccountName: minio-provisioner-lokik8sminio
      terminationGracePeriodSeconds: 0
      tolerations:
      - effect: NoSchedule
        key: infra
        operator: Equal
        value: reserved
      - effect: NoExecute
        key: infra
        operator: Equal
        value: reserved
      volumes:
      - emptyDir: {}
        name: empty-dir
      - configMap:
          defaultMode: 420
          name: minio-provisioner-lokik8sminio
        name: minio-provisioning
      - name: minio-credentials
        secret:
          defaultMode: 420
          secretName: minio-provisioner-lokik8sminio
